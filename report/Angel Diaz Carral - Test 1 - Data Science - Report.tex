\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{geometry}
\geometry{a4paper}

% Set up header/footer
\pagestyle{fancy}
\fancyhead[L]{Test Data Science 1 - Classification}
\fancyhead[R]{Ángel Díaz Carral}

\title{Test Data Science 1 - Classification}
\author{Ángel Díaz Carral}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report describes the classification project based on an open dataset. The project includes an exploratory data analysis (EDA), the training and evaluation of machine learning models, and the justification of the chosen model based on performance metrics. The findings and conclusions from the project are presented, along with the necessary deliverables to reproduce the analysis and deploy the trained model.
\end{abstract}

\section{Introduction}
In this project, we selected an open classification dataset (not the Iris dataset) from [source of dataset]. The goal is to perform an in-depth analysis, train and evaluate models, and justify the chosen models' performance based on several metrics.

\section{Exploratory Data Analysis (EDA)}

\subsection{Data Loading}
First, we load the dataset and inspect its structure to understand the features and target variables.

\begin{verbatim}
# Code to load the dataset
import pandas as pd
data = pd.read_csv('path_to_dataset.csv')
print(data.head())
\end{verbatim}

\subsection{Data Cleaning and Preprocessing}
This section covers the steps taken to clean and preprocess the data, such as handling missing values, encoding categorical variables, and normalizing/standardizing numerical features.

\begin{verbatim}
# Data cleaning and preprocessing steps
data.dropna(inplace=True)
data['categorical_feature'] = data['categorical_feature'].astype('category').cat.codes
\end{verbatim}

\subsection{Visualizations}
We perform several visualizations to better understand the data. Examples include histograms, boxplots, and pairplots.

%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.7\textwidth]{path_to_visualization.png}
%    \caption{Visualization of feature distributions}
%\end{figure}

\section{Model Selection and Training}

\subsection{Model Selection}
We select the model(s) based on the nature of the data and the problem. In this case, we experimented with the following models:

\begin{itemize}
    \item Logistic Regression
    \item Convolutional Neural Networks (CNN)
    \item Rotational Equivariant CNN
\end{itemize}

The model chosen was [chosen model], as it demonstrated the best performance according to [chosen metric].

\subsection{Model Training}
The following steps were performed to train the model:

\begin{verbatim}
# Code to train the model
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = data.drop(columns='target')
y = data['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)
\end{verbatim}

\section{Model Evaluation}

\subsection{Performance Metrics}
We evaluated the models using several performance metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.

\begin{verbatim}
# Evaluation code
from sklearn.metrics import accuracy_score, precision_score, recall_score

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='binary')
recall = recall_score(y_test, y_pred, average='binary')
f1 = 2 * (precision * recall) / (precision + recall)
\end{verbatim}

The performance results for the different models are as follows:

\begin{itemize}
    \item Logistic Regression: Accuracy = 0.85, F1 Score = 0.83
    \item CNN: Accuracy = 0.88, F1 Score = 0.85
    \item Rotational CNN: Accuracy = 0.90, F1 Score = 0.88
\end{itemize}

\section{Conclusions}

Based on the results, we conclude that [chosen model] outperforms the other models in terms of [metric]. The model is robust and achieves a good balance between precision and recall, making it suitable for deployment.

\section{Deliverables}

The following files are provided to reproduce the analysis and deploy the trained model:

\begin{description}
    \item[\texttt{train\_model.py}] The script for training the model
    \item[\texttt{evaluate\_model.py}] The script for evaluating the model
    \item[\texttt{model.pkl}] The trained model in pickle format
    \item[\texttt{structure.txt}] Directory structure of the project
\end{description}

Additionally, you can run the inference using the FastAPI-based API available at [API endpoint URL].

\section{References}
\begin{enumerate}
    \item Dataset source
    \item Paper/Book for model selection
\end{enumerate}

\end{document}
